{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxlgMrYqYyHQOBsV1fFQeL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wheemin-2/25-1-ESAA/blob/main/0512_HW_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **딥러닝**"
      ],
      "metadata": {
        "id": "c5YFd2MAQtVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "퍼셉트론 : 신경망(딥러닝)의 기원이 되는 알고리즘\n",
        "- 다수의 (흐름이 있는) 신호를 입력으로 받아 하나의 신호를 출력하는데, 이 신호를 입력으로 받아 '흐른다/안 흐른다(1 또는 0)'는 정보를 앞으로 전달하는 원리로 작동함\n",
        "\n",
        " ![perceptron](https://thebook.io/img/080289/138.jpg)\n",
        "\n",
        "\n",
        "컴퓨터가 두 개의 입력 $(x_1, x_2)$을 논리적으로 인식하는 방식\n",
        "\n",
        "1. AND 게이트 : 모든 입력이 '1'일 때 작동\n",
        "2. OR 게이트 : 입력 모두가 '0'인 경우를 제외한 나머지 상황에서 작동\n",
        "3. XOR 게이트 : 두 개 중 한 개만 '1'일 떄 작동하는 논리 연산 (배타적 논리합)\n",
        "![XOR](https://thebook.io/img/080289/140.jpg)\n",
        "\n",
        "- XOR 게이트는 데이터가 비선형적으로 분리되기 때문에 제대로 된 분류가 어려움\n",
        "- 단층 퍼셉트론에서는 AND, OR 연산에 대해 학습이 가능하지만 XOR에 대해서는 학습이 불가능\n",
        "- 입력층과 출력층 사이에 하나 이상의 은닉층을 두는 '다중 퍼셉트론'을 고안함\n",
        "    - 은닉층이 여러 개 있는 신경망을 DNN, DL 이라고 함"
      ],
      "metadata": {
        "id": "C6ZHff99QyS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **딥러닝 구조**"
      ],
      "metadata": {
        "id": "q8Veofk9SOFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **딥러닝 용어**"
      ],
      "metadata": {
        "id": "59I23XP3SonZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥러닝은 입력층, 출력층과 두 개 이상의 은닉층으로 구성됨 + 입력 신호를 전달하기 위한 다양한 함수 사용\n",
        "\n",
        "![DL](https://thebook.io/img/080289/141.jpg)\n",
        "\n",
        "- **입력층 (input layer)** : 데이터를 받아들이는 층\n",
        "- **은닉층 (hidden layer)** : 모든 입력 노드로부터 입력 값을 받아 가중합을 계산하고, 이 값을 활성화 함수에 적용하여 출력층에 전달하는 층\n",
        "- **출력층 (output layer)** : 신경망의 최종 결과값이 포함된 층\n",
        "- **가중치 (weight)** : 노드와 노드 간 연결 강도\n",
        "- **가중합 (wieghted sum)** : 가중치와 신호의 곱을 합한 것\n",
        "- **bias** : 가중합에 더해주는 상수로, 하나의 뉴런에서 활성화 함수를 거쳐 최종적으로 출력되는 값을 조절하는 역할\n",
        "- **활성화 함수 (activation function)** : 신호를 입력받아 이를 적절히 처리하여 출력해주는 함수\n",
        "- **손실 함수 (loss function)** : 가중치 학습을 위해 출력 함수의 결과와 실제값 간의 오차를 측정하는 함수"
      ],
      "metadata": {
        "id": "mpaxulGMS0Sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**가중치**\n",
        "\n",
        "- 입력 값이 연산 결과에 미치는 영향력을 조절하는 요소\n",
        "- 노드 신호가 아무리 높아도 가중치가 0이거나 0에 가깝다면 $x_1$x$w_1$ 값도 0이거나 0에 가까운 값이 됨\n",
        "\n",
        "**가중합 (또는 전달 함수)**\n",
        "\n",
        "- 각 노드에서 들어오는 신호에 가중치를 곱한 값의 합계\n",
        "\n",
        "- 노드의 가중합이 계산되면 가중합을 활성화 함수로 보내기 때문에 전달 함수(transfer function)라고도 함\n",
        "\n",
        " ![weighted sum](https://thebook.io/img/080289/fn-8.jpg)\n",
        "\n",
        "![weight](https://thebook.io/img/080289/142.jpg)\n",
        "\n",
        "![weighted sum1](https://thebook.io/img/080289/143.jpg)"
      ],
      "metadata": {
        "id": "KalH0CeKT-Hz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**활성화 함수**\n",
        "- 전달 함수에서 전달받은 값을 출력할 때 일정 기준에 따라 출력 값을 변화시키는 비선형 함수\n",
        "- Sigmoid, hyperbolic tangent, ReLU 등이 있음\n",
        "\n",
        "**[시그모이드 함수 (Sigmoid)]**\n",
        "- 선형 함수의 결과를 0~1 사이에서 비선형 형태로 변형해줌\n",
        "- 주로 로지스틱 회귀와 같은 분류 문제를 확률적으로 표현하는 데 사용됨\n",
        "- 딥러닝 모델의 깊이가 깊어지면 기울기가 사라지는 '기울기 소멸 문제 (Vanishing Gradient Problem)' 발생 > 딥러닝 모델에서는 잘 사용하지 X\n",
        "\n",
        "$$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "  \n",
        "  ![sigmoid](https://thebook.io/img/080289/144_1.jpg)\n",
        "\n",
        "**[하이퍼볼릭 탄젠트 함수]**\n",
        "- 선형 함수의 결과를 -1~1 사이에서 비 선형 형태로 변형해줌\n",
        "- 시그모이드에서 결과값의 평균이 0이 아닌 양수로 편향된 문제를 해결하는 데 사용했으나 기울기 소멸 문제는 여전히 발생\n",
        " ![hyperbolic tangent](https://thebook.io/img/080289/144_2.jpg)\n",
        "\n",
        "**[렐루 함수]**\n",
        "- 최근 활발히 사용됨\n",
        "- 입력(x)이 음수일 때는 0을 출력, 양수일 때는 x를 출력함\n",
        "- 경사 하강법에 영향을 주지 않아 학습 속도가 빠르고, 기울기 소멸 문제가 발생하지 않음\n",
        "- 일반적으로 은닉층에서 사용되며, 하이퍼볼릭 탄젠트 함수 대비 학습 속도가 6배 빠름\n",
        "- Leaky ReLU 함수 : 음수 값을 입력받으면 항상 0을 출력한다는 렐루 함수의 단점을 보완\n",
        " ![ReLU](https://thebook.io/img/080289/144_3.jpg)\n",
        "\n",
        "**[리키 렐루 함수]**\n",
        "- 입력값이 음수이면 0이 아닌 0.001처럼 매우 작은 수를 반환함\n",
        "- 입력값이 수렴하는 구간이 제거되어 렐루 함수를 사용할 때 생기는 문제를 해결할 수 있음\n",
        " ![Leaky ReLU](https://thebook.io/img/080289/145.jpg)\n",
        "\n",
        "**[소프트맥스 함수]**\n",
        "- 입력값을 0~1 사이에 출력되도록 정규화하여 출력 값들의 총합이 항상 1이 되도록 함\n",
        "- 보통 딥러닝에서 출력 노드의 활성화 함수로 많이 사용됨\n",
        " ![SOFTMAX](https://thebook.io/img/080289/fn-28.jpg)\n",
        "- n : 출력층의 뉴런 개수\n",
        "- $y_k$ : k 번째 출력\n",
        "- $a_k$ : 입력 신호\n"
      ],
      "metadata": {
        "id": "2nK39EOcVfkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "AKLgatHZZ_-K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p3026biIzlYT"
      },
      "outputs": [],
      "source": [
        "# ReLU, Softmax 구현\n",
        "class Net(torch.nn.Module):\n",
        "    def __init(self, n_feature, n_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
        "        self.relu = torch.nn.ReLu (inplace=True)\n",
        "        self.out = torch.nn.Linear(n_hidden, n_output)\n",
        "        self.softmax = torch.nn.Softmax(dim=n_output)\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.out(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**손실 함수**\n",
        "\n",
        "경사 하강법은 학습률(learning rate)과 손실 함수의 순간 기울기를 이용하여 가중치를 업데이트함. 즉, 미분의 기울기를 이용하여 오차를 비교하고 최소화하는 방향으로 이동시키는 방법 >  이때 오차를 구하는 방법이 손실 함수!\n",
        "\n",
        "즉, 손실 함수는 학습을 통해 얻은 데이터의 추정치가 실제 데이터와 얼마나 차이가 나는지 평가하는 지표라고 할 수 있음.\n",
        "- 값이 클수록 많이 틀렸다는 의미이며, ‘0’에 가까우면 완벽하게 추정할 수 있다는 의미\n",
        "- 대표적인 손실 함수 : 평균 제곱 오차(Mean Squared Error, MSE)와 크로스 엔트로피 오차(Cross Entropy Error, CEE)"
      ],
      "metadata": {
        "id": "3Tzn1XRFaC5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[평균 제곱 오차, MSE]**\n",
        "- 실제 값과 예측값의 차이(error)를 제곱하여 평균을 낸 것\n",
        "\n",
        "\n",
        "```\n",
        "# 파이토치에서 mse 사용\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "y_pred = model(x)\n",
        "loss = loss_fn(y_pred, y)\n",
        "```\n",
        "\n",
        "\n",
        " ![mse](https://thebook.io/img/080289/fn-9.jpg)"
      ],
      "metadata": {
        "id": "lFf8pH8Rbhi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[크로스 엔트로피 오차]**\n",
        "- 분류 문제에서 원-핫 인코딩 했을 때 사용할 수 있는 오차 계산법\n",
        "- MSE와 시그모이드를 결합하여 사용하면 시그모이드의 특성으로 gradient가 매끄럽지 못한 울퉁불퉁한 상태이며 학습 속도도 매우 늦음 > 이를 극복한 것이 크로스 엔트로피 오차!\n",
        "- 크로스 엔트로피 오차는 두 개의 확률 분포 차이를 이용하므로 시그모이드의 영향을 덜 받음 > 학습 속도가 빠름\n",
        "\n",
        "\n",
        "```\n",
        "loss = nn.CrossEntropyLoss()\n",
        "# torch.randn : 평균이 0, sd가 1인 가우시안 정규분포를 이용해 숫자 생성\n",
        "input = torch.randn(5,6, requires_grad=True)  \n",
        "# torch.empty : 랜덤한 값으로 채워진 텐서 반환\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "output = loss(input, target)\n",
        "output.backward()\n",
        "```\n",
        "\n",
        "\n",
        "![cross entropy](https://thebook.io/img/080289/fn-10.jpg)"
      ],
      "metadata": {
        "id": "9X0g7zlYb8fD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **딥러닝 학습**"
      ],
      "metadata": {
        "id": "SxYlRnVUdEsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥러닝 학습은 크게 순전파와 역전파, 두 단계로 진행됨"
      ],
      "metadata": {
        "id": "BElMnXt5dI0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![순전파/역전파](https://thebook.io/img/080289/148.jpg)"
      ],
      "metadata": {
        "id": "xyVYk6XIdT2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**순전파**\n",
        "- 네트워크에 훈련 데이터가 들어올 때 발생, 데이터를 기반으로 예측 값을 계산하기 위해 전체 신경망을 교차해 나감\n",
        "- 모든 뉴런이 이전 층의 뉴런에서 수신한 정보에 변환(가중합 및 활성화 함수)을 적용하여 다음 층(은닉층)의 뉴런으로 전달하는 방식\n",
        "- 네트워크를 통해 입력 데이터 전달, 데이터가 모든 층을 통과하고 모든 뉴런이 계산을 완료하면 예측값은 최종층(출력층)에 도달하게 됨\n",
        "- 손실 함수로 네트워크의 예측 값, 실제 값의 차이(손실, 오차)를 추정함. 이때 손실 함수 비용은 0이 이상적임 → 손실 함수 비용이 0에 가깝도록 하기 위해 모델이 훈련을 반복하면서 가중치를 조정\n",
        "\n",
        "**역전파**\n",
        "- 손실(오차)이 계산되면 정보는 역으로 전파(출력층 → 은닉층 → 입력층)되기 때문에 역전파라 불림\n",
        "- 예측 값과 실제 값 차이를 각 뉴런의 가중치로 미분한 후 기존 가중치 값에서 뺌\n",
        "- 이 과정을 출력층 → 은닉층 → 입력층 순서로 모든 뉴런에 대해 진행하여 계산된 각 뉴런 결과를 또다시 순전파의 가중치 값으로 사용"
      ],
      "metadata": {
        "id": "da_dpZ5-fHTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **딥러닝의 문제점과 해결방안**"
      ],
      "metadata": {
        "id": "AH4kn4SGfPce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥러닝의 핵심은 활성화 함수가 적용된 여러 은닉층을 결합하여 비선형 영역을 표현하는 것!\n",
        "\n",
        "활성화 함수가 적용된 은닉층 개수가 많을 수록 데이터 분류가 잘 되지만, 그에 따른 문제 발생\n",
        "\n",
        "### 1. 과적합 문제 발생\n",
        "- 훈련 데이터에 대해 과하게 학습하여 실제 데이터에 대한 오차가 증가\n",
        "- 이를 방지하기 위해 Dropout을 적용 : 임의로 일부 노드들을 학습에서 제외시킴\n",
        " ![dropout](https://thebook.io/img/080289/150.jpg)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vqciKfEJfoZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이토치에서 dropout 구현\n",
        "\n",
        "class DropoutModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DropoutModel, self).__init__()\n",
        "        self.layer1 = torch.nn.Linear(784, 1200)\n",
        "        self.dropout1 = torch.nn.Dropout(0.5)\n",
        "        self.layer2 = torch.nn.Linear(1200, 1200)\n",
        "        self.dropout2 = torch.nn.Dropout(0.5)\n",
        "        self.layer3 = torch.nn.Linear(1200, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.dropout2(x)\n",
        "        return self.layer3(x)"
      ],
      "metadata": {
        "id": "cppsSeKpZ9Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 기울기 소멸 문제 발생\n",
        "- 은닉층이 많은 신경망에서 주로 발생, 출력층에서 은닉층으로 전달되는 오차가 크게 줄어들어 학습이 되지 않는 현상\n",
        "- 기울기가 소멸되기 때문에 학습되는 양이 '0'에 가까워져 학습이 더디게 진행되다 오차를 더 줄이지 못하고 그 상태로 수렴하는 현상\n",
        "- 시그모이드, 하이퍼볼릭 탄젠트 << *렐루 활성화 함수를 사용하면* 해결 가능"
      ],
      "metadata": {
        "id": "_W8lbxmMhS4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 성능 저하 문제 발생\n",
        "\n",
        "- 경사 하강법은 손실 함수의 비용이 최소가 되는 지점을 찾을 때까지 기울기가 낮은 쪽으로 계속 이동시키는 과정을 반복하는데, 이 때 성능이 나빠지는 문제 발생\n",
        "- 미니 배치 경사 하강법 / 확률적 경사 하강법으로 개선 가능\n",
        "\n",
        " ![gradient descent](https://thebook.io/img/080289/152_1.jpg)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.x_data = [[1,2,3],[4,5,6],[7,8,9]]\n",
        "        self.y_data = [[12],[18],[11]]\n",
        "        def __len__(self):\n",
        "            return len(self.x_data)\n",
        "        def __getitem__(self, idx):\n",
        "            x = torch.FloatTensor(self.x_data[idx])\n",
        "            y = torch.FloatTensor(self.y_data[idx])\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "9zyWpZFFiCJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **딥러닝 알고리즘**"
      ],
      "metadata": {
        "id": "bD0QeYMnjs6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "딥러닝 알고리즘의 공통점 : 심층 신경망을 사용한다! 목적에 따라 합성곱 신경망(CNN), 순환 신경망(RNN), 제한된 볼츠만 머신(RBM), 심층 신뢰 신경망(DBN)으로 분류"
      ],
      "metadata": {
        "id": "DiC5U4L3kEB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **심층 신경망(DNN)**"
      ],
      "metadata": {
        "id": "YS-iu1g8jwzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력층과 출력층 사이에 다수의 은닉층을 포함하는 인공 신경망\n",
        "\n",
        "- 장점 : 다수의 은닉층을 두었기 때문에 다양한 비선형적 관계를 학습할 수 있음\n",
        "- 단점 : 학습을 위한 연산량이 많고, 기울기 소멸 문제 등이 발생할 수 있음\n",
        "    -  드롭아웃, 렐루 함수, 배치 정규화 등을 적용해 해결\n",
        "\n",
        "![DNN](https://thebook.io/img/080289/159_1.jpg)"
      ],
      "metadata": {
        "id": "EoAN2Sw7kYIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **합성곱 신경망(CNN)**"
      ],
      "metadata": {
        "id": "qHolLaq_kr1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ": 합성곱층(convolutional layer)과 풀링층(pooling layer)을 포함하는 이미지 처리 성능이 좋은 인공 신경망 알고리즘\n",
        "- 영상 및 사진이 포함된 이미지 데이터에서 객체를 탐색하거나 객체 위치를 찾아내는 데 유용!\n",
        "- 대표적인 CNN : LeNet-5, AlexNet, VGG, GoogLeNet, ResNet\n",
        "\n",
        "*기존 신경망과의 차별성*\n",
        "- 각 층의 입츌력 형상을 유지\n",
        "- 이미지의 공간 정보를 유지하면서 인접 이미지와 차이가 있는 특징을 효과적으로 인식\n",
        "- 복수 필터로 이미지의 특징을 추출, 학습\n",
        "- 추출한 이미지의 특징을 모으고 강화하는 풀링층 존재\n",
        "- 필터를 공유 파라미터로 사용하므로 일반 인공 신경망과 비교하여 학습 파라미터가 매우 적음\n",
        "\n",
        "![CNN](https://thebook.io/img/080289/159_2.jpg)"
      ],
      "metadata": {
        "id": "Ha98WQoWk2jU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **순환 신경망(RNN)**"
      ],
      "metadata": {
        "id": "3oH80Q1ylejG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ": 시계열 데이터(음악, 영상 등) 같은 시간 흐름에 따라 변화하는 데이터를 학습하기 위한 인공 신경망\n",
        "\n",
        "- 순환 신경망의 '순환(recurrent)' : 자기 자신을 참조한다! 즉, 현재 결과가 이전 결과와 연관이 있다는 의미\n",
        "- 자연어 처리 분야와 궁합이 잘 맞음\n",
        "    - 언어 모델링, 텍스트 생성, 자동 번역, 음성 인식, 이미지 캡션 생성\n",
        "\n",
        "*순환 신경망의 특징*\n",
        "- 시간성(temporal property)을 가진 데이터가 많음\n",
        "- 시간성 정보를 이용하여 데이터의 특징을 잘 다룸\n",
        "- 시간에 따라 내용이 변하므로 데이터는 동적이고, 길이가 가변적\n",
        "- 매우 긴 데이터를 처리하는 연구가 활발히 진행 중\n",
        "\n",
        "*한계*\n",
        "- 기울기 소멸 문제로 학습이 제대로 되지 않는 문제가 있으나, LSTM으로 해결 가능\n",
        "\n",
        "![RNN](https://thebook.io/img/080289/160.jpg)"
      ],
      "metadata": {
        "id": "G62-lL4vm43V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **제한된 볼츠만 머신(RBM)**"
      ],
      "metadata": {
        "id": "QA2VZM7gmx06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ": 가시층(visible layer)과 은닉층(hidden layer)으로 구성된 모델\n",
        "- 가시층은 은닉층과만 연결! (가시층과 가시층, 은닉층과 은닉층 사이에 연결은 없음)\n",
        "\n",
        "*제한된 볼츠만 머신의 특징*\n",
        "- 차원 감소, 분류, 선형 회귀 분석, 협업 필터링, 특성값 학습, 주제 모델링에 사용함\n",
        "- 기울기 소멸 문제를 해결하기 위해 사전 학습 용도로 활용 가능\n",
        "- 심층 신뢰 신경망(DBN)의 요소로 활용됨\n",
        "\n",
        " ![RBM](https://thebook.io/img/080289/161.jpg)"
      ],
      "metadata": {
        "id": "MHklmcKWn7Az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **심층 신뢰 신경망(DBN)**\n"
      ],
      "metadata": {
        "id": "yd7y69ayoYA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ": 입력층과 은닉층으로 구성된 제한된 볼츠만 머신을 블록처럼 여러 층으로 쌓은 형태로 연결된 신경망\n",
        "\n",
        "- 레이블이 없는 데이터에 대한 비지도 학습 가능\n",
        "- 부분적인 이미지에서 전체를 연상하는 일반화와 추상화 과정을 구현할 때 유용하게 사용 가능\n",
        "\n",
        "*심층 신뢰 신경망의 학습 절차*\n",
        "1. 가시층과 은닉층 1에 제한된 볼프만 머신을 사전 훈련\n",
        "2. 첫 번째 층 입력 데이터와 파라미터를 고정하여 두 번째 층 제한된 볼츠만 머신을 사전 훈련\n",
        "3. 원하는 개수만큼 제한된 볼츠만 머신을 쌓아올려 전체 DBN 완성\n",
        "\n",
        "*심층 신뢰 신경망의 특징*\n",
        "- 순차적으로 RBM을 학습시키며 계층적 구조를 생성함\n",
        "- 비지도 학습으로 학습함\n",
        "- 위로 올라갈수록 추상적 특성을 추출\n",
        "- 학습된 가중치를 다층 퍼셉트론의 가중치 초기값으로 사용\n",
        "\n",
        "![DBN](https://thebook.io/img/080289/162.jpg)"
      ],
      "metadata": {
        "id": "pjtykYRSqApF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLKziRC1m0fF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
